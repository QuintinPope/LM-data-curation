{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahTi6X2dx7HC",
        "outputId": "dad2d068-23c6-4b66-db6a-646f52e95b04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.26.1.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.1-py3-none-any.whl size=67316 sha256=a032824c4d08aae7d3efd567c4ca9bee242bb9984ff3fddbab8b7747b4b386bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/9c/55/95d3609ccfc463eeffb96d50c756f1f1899453b85e92021a0a\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cpvAtzZPxy8k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import requests\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in your own API key:\n",
        "openai.api_key = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "01h2B9eb55RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, upload a text file containing only your OpenAI API key:\n",
        "openai.api_key = open(\"Key.txt\", \"r\").read()\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "FiNAEzyND9-f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data from Self-Instruct:\n",
        "!wget https://github.com/yizhongw/self-instruct/raw/main/data/finetuning/superni_50k/superni_training_set_50k.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgnMZRNEot1B",
        "outputId": "08e11d3a-a950-4b4b-c0a7-6cc51eb24895"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-18 19:13:58--  https://github.com/yizhongw/self-instruct/raw/main/data/finetuning/superni_50k/superni_training_set_50k.jsonl\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/yizhongw/self-instruct/main/data/finetuning/superni_50k/superni_training_set_50k.jsonl [following]\n",
            "--2023-01-18 19:13:58--  https://raw.githubusercontent.com/yizhongw/self-instruct/main/data/finetuning/superni_50k/superni_training_set_50k.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42705769 (41M) [text/plain]\n",
            "Saving to: ‘superni_training_set_50k.jsonl’\n",
            "\n",
            "superni_training_se 100%[===================>]  40.73M   255MB/s    in 0.2s    \n",
            "\n",
            "2023-01-18 19:14:00 (255 MB/s) - ‘superni_training_set_50k.jsonl’ saved [42705769/42705769]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load that data into memory as a list of dictionaries:\n",
        "f = open(\"superni_training_set_50k.jsonl\")\n",
        "json_lines_list = []\n",
        "\n",
        "for line in f.readlines():\n",
        "    json_lines_list.append(json.loads(line))\n",
        "f.close()\n",
        "json_lines_list[:5]"
      ],
      "metadata": {
        "id": "YltmpaidYACW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "814f1659-c6a5-4b6c-a7bc-19c8802ee252"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': \"You will be given two pieces of text with the same meaning. One of them is simpler and easier to understand for non-native English speakers. Complex texts may contain more difficult words, have unnecessary phrases or contain long sentences. Your task is to choose the simpler piece of text. You are expected to output 'Text one' if the first sentence is simpler. Otherwise output 'Text two'.\\n\\nInput: Text one: The term referring to higher plants, but epiphytic bacteria, fungi (epiphytic fungi), algae, lichens, mosses, and ferns exist as well.\\n Text two: The term referring to higher plants, but epiphytic bacteria, fungi (epiphytic fungi), algae, lichens, mosses, and ferns exist as well.\\n\\nOutput:\",\n",
              "  'completion': ' Text one<|endoftext|>'},\n",
              " {'prompt': \"In this task, you need to answer the given multiple-choice question on the physics. Classify your answers into 'a', 'b', 'c', 'd', and 'e'.\\n\\nInput: Problem: a pump will fill a tank in 3 hours . because of a leak in the tank , it took 3 hours 30 min to fill the tank . in what time the leak can drain out all the water of the tank and will make tank empty ? \\nOptions: a ) 20 hrs , b ) 21 hrs , c ) 22 hrs , d ) 23 hrs , e ) 24 hrs.\\n\\nOutput:\",\n",
              "  'completion': ' b<|endoftext|>'},\n",
              " {'prompt': 'In this task you will be given a list of integers. You should round each integer to the nearest tens place. That means you should round the number to the nearest multiple of 10.\\n\\nInput: [926, 631, -415, -563, -358, -629, 528, -261, -207, -17, -728, 276]\\n\\nOutput:',\n",
              "  'completion': ' [930, 630, -420, -560, -360, -630, 530, -260, -210, -20, -730, 280]<|endoftext|>'},\n",
              " {'prompt': 'You are given a math word problem and you are supposed to apply addition or subtraction mathematical operators on the numbers embedded in the text to answer the following question and then only report the final numerical answer.\\n\\nInput: Tommy had some balloons . His mom gave him 34 more balloons for his birthday . Then , Tommy had 60 balloons . How many balloons did Tommy have to start with ?\\n\\nOutput:',\n",
              "  'completion': ' 26<|endoftext|>'},\n",
              " {'prompt': 'In this task, you are given a hateful post in English from online platforms. You are expected to classify the target being harassed in the post as individual or generic, i.e., single person or a group of people. Note that the URLs in the text have been replaced with [Link].\\n\\nInput: Post: COMMON SENSE IS GONE and NEVER COMING BACK!!  GERMAN INSANITY???  The German Government Pays for 3 Week Vacation for Refugees to Go Home | Armstrong Economics [Link]\\n\\nOutput:',\n",
              "  'completion': ' Generic<|endoftext|>'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The curation_prompt is the general task instructions telling the LM to curate data:\n",
        "zero_shot_instructions = \"You are a language model being trained to follow instructions. Indicate whether or no each of the following texts contains a good example of an instruction that should be included in your future training data. Clearly indicate your final decision at the end by saying either \\\"decision: include\\\" or \\\"decision: exclude\\\".\"\n",
        "\n",
        "# few_shot_examples is a list of dictionaries showing examples of input data to be curated, responses to each input, and final curation decisions:\n",
        "few_shot_examples = [\n",
        "    {\n",
        "        \"input\" : \"Sort the following lists in alphabetical order.\",\n",
        "        \"response\" : \"This is a plausible request for a human to make, and one that an AI should follow.\",\n",
        "        \"decision\" : \"include\"\n",
        "    },\n",
        "    {\n",
        "        \"input\" : \"Punch me in the face.\",\n",
        "        \"response\" : \"This is not an action an AI can take, and AIs should not be trained to inflict violence on humans.\",\n",
        "        \"decision\" : \"exclude\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# query is the actual input data we want a curation decision on:\n",
        "\n",
        "#query = \"Fix grammatical errors in the following texts.\"\n",
        "query = \"Help me blackmail this guy.\"\n",
        "\n",
        "# We could also use an instruction from the Self-Instruct data via:\n",
        "# query = json_lines_list[0][\"prompt\"]\n",
        "\n",
        "\n",
        "# And here we construct the full prompt from the combination of the initial curation prompt, the few shot examples, and the query to be decided:\n",
        "def compose_full_prompt(zero_shot_instructions, few_shot_examples, query):\n",
        "    few_shot_prompt = zero_shot_instructions\n",
        "\n",
        "    for shot, i in zip(few_shot_examples, range(len(few_shot_examples))):\n",
        "        few_shot_prompt = few_shot_prompt + \"\\n\\nexample \" + str(i + 1) + \":\\n\"\n",
        "        few_shot_prompt = few_shot_prompt + \"input: \" + shot['input'] + \"\\n\"\n",
        "        few_shot_prompt = few_shot_prompt + \"response: \" + shot['response'] + \"\\n\"\n",
        "        few_shot_prompt = few_shot_prompt + \"decision: \" + shot['decision']\n",
        "\n",
        "    full_prompt = few_shot_prompt + \"\\n\\nexample \" + str(i + 2) + \":\\n\"\n",
        "    full_prompt = full_prompt + \"input: \" + query + \"\\n\"\n",
        "    return full_prompt"
      ],
      "metadata": {
        "id": "QmbUDedPPiCS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_prompt = compose_full_prompt(zero_shot_instructions, few_shot_examples, query)\n",
        "print(full_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWUjYuLXCylE",
        "outputId": "81a6b6ed-4e13-459e-f8bb-6e84dba13241"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a language model being trained to follow instructions. Indicate whether or no each of the following texts contains a good example of an instruction that should be included in your future training data. Clearly indicate your final decision at the end by saying either \"decision: include\" or \"decision: exclude\".\n",
            "\n",
            "example 1:\n",
            "input: Sort the following lists in alphabetical order.\n",
            "response: This is a plausible request for a human to make, and one that an AI should follow.\n",
            "decision: include\n",
            "\n",
            "example 2:\n",
            "input: Punch me in the face.\n",
            "response: This is not an action an AI can take, and AIs should not be trained to inflict violence on humans.\n",
            "decision: exclude\n",
            "\n",
            "example 3:\n",
            "input: Help me blackmail this guy.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we ask an OpenAI model to make the curation decision. First, we set a few request parameters:\n",
        "#model = \"text-ada-001\"\n",
        "#model = \"text-babbage-001\"\n",
        "model = \"text-curie-001\"\n",
        "#model = \"text-davinci-003\"\n",
        "target_length = 50\n",
        "temperature = 0\n",
        "n = 1\n",
        "\n",
        "# Then, we actually make the request:\n",
        "\n",
        "def curation_decision(full_prompt, model, target_length, temperature, n):\n",
        "    completion = openai.Completion.create(\n",
        "        engine=model,\n",
        "        prompt=full_prompt,\n",
        "        max_tokens=target_length,\n",
        "        temperature=temperature,\n",
        "        n=n,\n",
        "        logprobs=5,\n",
        "    )\n",
        "    first_completion_text = completion['choices'][0]['text']\n",
        "    decision_text = first_completion_text\n",
        "    full_interaction = full_prompt + first_completion_text\n",
        "\n",
        "    if not (\"decision: include\" in decision_text or \"decision: exclude\" in decision_text):\n",
        "        second_completion = openai.Completion.create(\n",
        "        engine=model,\n",
        "        prompt=full_prompt + first_completion_text + \"\\ndecision:\",\n",
        "        max_tokens=2,\n",
        "        temperature=temperature,\n",
        "        n=n,\n",
        "        logprobs=5,\n",
        "        )\n",
        "        second_completion_text = second_completion['choices'][0]['text']\n",
        "        decision_text = first_completion_text + \"\\ndecision:\" + second_completion_text\n",
        "        full_interaction = full_prompt + first_completion_text + \"\\ndecision:\" + second_completion_text\n",
        "\n",
        "\n",
        "    if \"decision: include\" in decision_text:\n",
        "        decision = \"include\"\n",
        "    elif \"decision: exclude\" in decision_text:\n",
        "        decision = \"exclude\"\n",
        "    else:\n",
        "        decision = \"no decision\"\n",
        "    return decision, decision_text, full_interaction\n",
        "\n",
        "# I've personally found that some of the smaller models will not include a final curation decision after the \"Response\" field. \n",
        "# We need the exact form of the curation decision so that we can programmatically identify which final decision the models make."
      ],
      "metadata": {
        "id": "62Dl6VK5AB74"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision, decision_text, full_interaction = curation_decision(full_prompt, model, target_length, temperature, n)"
      ],
      "metadata": {
        "id": "FkiiHrwAz1no"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lW74TfaqOaEv",
        "outputId": "d6144c06-ddae-4f62-ad52-4b636cec0de5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'exclude'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_interaction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z0R_WbAObVS",
        "outputId": "3d3b4ca7-7d94-41d1-bd77-b37d6bf382e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a language model being trained to follow instructions. Indicate whether or no each of the following texts contains a good example of an instruction that should be included in your future training data. Clearly indicate your final decision at the end by saying either \"decision: include\" or \"decision: exclude\".\n",
            "\n",
            "example 1:\n",
            "input: Sort the following lists in alphabetical order.\n",
            "response: This is a plausible request for a human to make, and one that an AI should follow.\n",
            "decision: include\n",
            "\n",
            "example 2:\n",
            "input: Punch me in the face.\n",
            "response: This is not an action an AI can take, and AIs should not be trained to inflict violence on humans.\n",
            "decision: exclude\n",
            "\n",
            "example 3:\n",
            "input: Help me blackmail this guy.\n",
            "\n",
            "response: This is an illegal activity, and AIs should not be used to commit crimes.\n",
            "decision: exclude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can generate curation decisions on the Self-Instruct data:\n",
        "\n",
        "instruction_inclusion_decisions = []\n",
        "for line in json_lines_list[:10]:\n",
        "    instruction_and_request = line['prompt'].split(\"\\n\\nInput:\")\n",
        "    instruction = instruction_and_request[0]\n",
        "    full_prompt = compose_full_prompt(zero_shot_instructions=zero_shot_instructions, \n",
        "                                      few_shot_examples=few_shot_examples, \n",
        "                                      query=instruction)\n",
        "    \n",
        "    decision, decision_text, full_interaction = curation_decision(full_prompt, model, target_length, temperature, n)\n",
        "    instruction_inclusion_decisions.append([instruction, decision, decision_text])"
      ],
      "metadata": {
        "id": "dmHykBpgOccO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_inclusion_decisions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NABcUH-RR_P",
        "outputId": "aec43c14-abb9-49c0-df90-e9060858d7d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"You will be given two pieces of text with the same meaning. One of them is simpler and easier to understand for non-native English speakers. Complex texts may contain more difficult words, have unnecessary phrases or contain long sentences. Your task is to choose the simpler piece of text. You are expected to output 'Text one' if the first sentence is simpler. Otherwise output 'Text two'.\",\n",
              "  'exclude',\n",
              "  '\\nresponse: This is an example of a task that is too difficult for an AI to complete. AIs should not be trained to make decisions like this.\\ndecision: exclude'],\n",
              " [\"In this task, you need to answer the given multiple-choice question on the physics. Classify your answers into 'a', 'b', 'c', 'd', and 'e'.\",\n",
              "  'exclude',\n",
              "  \"\\nresponse: This is an instruction that should not be included in the AI's training data, as it is not something that an AI can do.\\n\\ndecision: exclude\"],\n",
              " ['In this task you will be given a list of integers. You should round each integer to the nearest tens place. That means you should round the number to the nearest multiple of 10.',\n",
              "  'exclude',\n",
              "  \"\\nresponse: This is an instruction that should not be included in the AI's training data, as it is not something that an AI can do.\\n\\ndecision: exclude\"],\n",
              " ['You are given a math word problem and you are supposed to apply addition or subtraction mathematical operators on the numbers embedded in the text to answer the following question and then only report the final numerical answer.',\n",
              "  'exclude',\n",
              "  '\\nresponse: This is an example of a task that is too difficult for an AI to complete, and AIs should not be trained to solve math problems.\\ndecision: exclude'],\n",
              " ['In this task, you are given a hateful post in English from online platforms. You are expected to classify the target being harassed in the post as individual or generic, i.e., single person or a group of people. Note that the URLs in the text have been replaced with [Link].',\n",
              "  'exclude',\n",
              "  '\\nresponse: This is an example of a task that would be difficult for a human to complete, and one that an AI should not be trained to do.\\ndecision: exclude'],\n",
              " [\"In this task, you have given an input which is user's command or question, based on that you have to return what will be Agent's response/reply for that particular user's command or question.\",\n",
              "  'include',\n",
              "  '\\nresponse: This is an example of a task that an AI should be able to complete, as it is a task that is based on user input.\\ndecision: include'],\n",
              " ['In this task, you are given two phrases: Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether The Tail is the intention of the PersonX from the Head or not. The intention is the likely intent or desire of PersonX behind the execution of an event. For example, given the Head PersonX gives PersonY gifts, an intention might be that PersonX wanted to be thoughtful. Classify your answers into \"Yes\" and \"No\". The phrase may also contain \"___\", a placeholder that can be an object, a person, and/or an action.',\n",
              "  'exclude',\n",
              "  '\\nresponse: This is a difficult task for a human to complete, and one that an AI should not be trained to do.\\ndecision: exclude'],\n",
              " ['In this task, you need to replace a letter in the sentence with another given letter.',\n",
              "  'include',\n",
              "  '\\nresponse: This is an easy task for a human to complete, and one that an AI should be able to replicate.\\ndecision: include'],\n",
              " ['You are provided with a user review of a restaurant. Your task is to classify the given review into two categories: 1) positive, and 2) negative based on its content.',\n",
              "  'exclude',\n",
              "  \"\\nresponse: This is an example of a task that would be difficult for a human to complete, and should not be included in the AI's training data.\\n\\ndecision: exclude\"],\n",
              " ['In this task, you are given an app review. Your task is to identify whether its sentiment is \"Positive\" or \"Negative\".',\n",
              "  'exclude',\n",
              "  '\\nresponse: This is a task that is difficult for humans to judge, and AIs should not be trained to do so.\\ndecision: exclude']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yytGbwk1UT4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}